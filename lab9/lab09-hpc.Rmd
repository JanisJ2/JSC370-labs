---
title: "Lab 9 - HPC"
output: 
html_document: default
link-citations: yes
---

# Learning goals

In this lab, you are expected to practice the following skills:

- Evaluate whether a problem can be parallelized or not.
- Practice with the parallel package.
- Use Rscript to submit jobs.

```{r eval=FALSE, echo=FALSE}
# install any missing packages
install.packages("microbenchmark")
```

## Problem 1

Give yourself a few minutes to think about what you learned about parallelization. List three
examples of problems that you believe may be solved using parallel computing,
and check for packages on the HPC CRAN task view that may be related to it.

- cross-validation in machine learning
- caret -> supports parallel cross-validation with `doParallel`
- mlr, foreach, doParallel -> for parallel model training

- bootstrapping
- boot -> for bootstrapping
- parallel -> parallelize resampling

- markov chain monte carlo
- parallel
- rstan -> for stan for bayesian modeling
- RcppParallel -> parallel mcmc sampling
- nimle -> customize bayesian inference

https://cran.r-project.org/web/views/HighPerformanceComputing.html


## Problem 2: Pre-parallelization

The following functions can be written to be more efficient without using
`parallel`:

1. This function generates a `n x k` dataset with all its entries having a Poisson distribution with mean `lambda`.

```{r p2-fun1}
fun1 <- function(n = 100, k = 4, lambda = 4) {
  x <- NULL
  
  for (i in 1:n)
    x <- rbind(x, rpois(k, lambda))
  
  return(x)
}

fun1alt <- function(n = 100, k = 4, lambda = 4) {
  # YOUR CODE HERE
  matrix(rpois(n*k, lambda=lambda), ncol=k)
  # allow us to pre-allocating memory
  # rpois(n*k, lambda=lambda) allows us to generate the random numbers all at once
}

# Benchmarking
microbenchmark::microbenchmark(
  fun1(),
  fun1alt(),
  unit = "ns"
)
```

How much faster?

_Answer here._
Using parallelization, our function `run1alt` runs about 10+ times faster than the original function `fun1`.


2.  Find the column max (hint: Checkout the function `max.col()`).

```{r p2-fun2}
# Data Generating Process (10 x 10,000 matrix)
set.seed(1234)
x <- matrix(rnorm(1e4), nrow=10)

# Find each column's max value
fun2 <- function(x) {
  apply(x, 2, max)
}

fun2alt <- function(x) {
  # YOUR CODE HERE
  x[cbind(max.col(t(x)), 1:ncol(x))]
  # avoids function calls inside loops
  # directly extracting the max values, without any loops
}

# Benchmarking
bench <- microbenchmark::microbenchmark(
  fun2(x),
  fun2alt(x),
  unit = "us"
)
```

```{r}
library(ggplot2)

plot(bench)
autoplot(bench) +
  theme_minimal() +
  labs(title = "Benchmarking fun2alt vs fun2",
       x = "Function",
       y = "Time (Âµs)") +
  theme_minimal()
```

The violin plot compares the execution time of `fun2(x)` (using `apply()`) and `fun2alt(x)` (using `max.col()`). The distribution of execution times for `fun2(x)` is significantly higher and more spread out, indicating that it is slower and more variable. In contrast, `fun2alt(x)` has a much lower execution time, with a more concentrated distribution near the lower end, suggesting it is consistently faster. This confirms that using `max.col()` is a more efficient approach for finding column-wise maximum values in a large matrix.


## Problem 3: Parallelize everything

We will now turn our attention to non-parametric 
[bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)).
Among its many uses, non-parametric bootstrapping allow us to obtain confidence
intervals for parameter estimates without relying on parametric assumptions.

The main assumption is that we can approximate many experiments by resampling
observations from our original dataset, which reflects the population. 

This function implements the non-parametric bootstrap:

```{r p3-boot-fun}
my_boot <- function(dat, stat, R, ncpus = 1L) {
  
  # Getting the random indices
  n <- nrow(dat)
  idx <- matrix(sample.int(n, n*R, TRUE), nrow=n, ncol=R)
 
  # Making the cluster using `ncpus`
  # STEP 1: GOES HERE
  cl <- makePSOCKcluster(ncpus)
  # creating cluster for parallel computing
  # ncpus specifying using multiple CPU cores
  # PSOCK parallel socket cluster
  # STEP 2: GOES HERE
  # on.exit(stopCluster(cl))
  # export the variables to the cluster
  clusterExport(cl, varlist = c("idx", "dat", "stat"), envir = environment())
  # sending the variables to all worker nodes
  # each run in isolated environment, don't have access to global variable
  # idx -> resampling indices for bootstrapping
  # dat -> dataset
  # stat -> the statistical function that we use to compute the estimates

  # change sequential apply to parallelized apply
  # STEP 3: THIS FUNCTION NEEDS TO BE REPLACED WITH parLapply
  ans <- parLapply(cl, seq_len(R), function(i) {
    stat(dat[idx[,i], , drop=FALSE])
  })
  
  # Coercing the list into a matrix
  ans <- do.call(rbind, ans)
  
  # STEP 4: GOES HERE
  stopCluster(cl)
  # why?
  # free up system resources
  
  ans
  
}
```

1. Use the previous pseudocode, and make it work with `parallel`. Here is just an example for you to try:

```{r p3-test-boot}
# Bootstrap of a linear regression model
library(parallel)
my_stat <- function(d) coef(lm(y~x, data = d)) 

# DATA SIM
set.seed(1)
n <- 500 
R <- 1e4
x <- cbind(rnorm(n)) 
y <- x*5 + rnorm(n)

# Check if we get something similar as lm
ans0 <- confint(lm (y~x))
cat("OLS CI \n")
ans0

ans1 <- my_boot(dat = data.frame(x, y), my_stat, R = R, ncpus = 4)
qs <- c(.025, .975)
cat("Bootstrap CI \n")
print(t(apply(ans1, 2, quantile, probs = qs)))
```

2. Check whether your version actually goes faster than the non-parallel version:

```{r benchmark-problem3}
# your code here
detectCores()

# non-parallel 1 core
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 1L))

# parallel 8 cores
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 8L))
```

The parallel version (ncpus = 8) is faster than the single-core version, reducing the elapsed time from 1.98 seconds to 1.72 seconds. This confirms that parallelization improves efficiency, but the speedup is not linear (i.e., it did not reduce the runtime by a factor of 8). The additional overhead from setting up and managing multiple worker processes accounts for this discrepancy. Nevertheless, parallelization provides a noticeable performance gain, making it beneficial for larger datasets or higher values of R.

## Problem 4: Compile this markdown document using Rscript

Once you have saved this Rmd file, try running the following command
in your terminal:

```bash
Rscript --vanilla -e 'rmarkdown::render("[full-path-to-your-Rmd-file.Rmd]")' &
```

Where `[full-path-to-your-Rmd-file.Rmd]` should be replace with the full path to
your Rmd file... :).


